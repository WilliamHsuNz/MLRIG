{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import generic_utils\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy \n",
    "import math\n",
    "import h5py\n",
    "import time\n",
    "import copy\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/whsu014/dev/mlrig/Experiments\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1011454928711085629\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4523506320290111442\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7801326797\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2279616840177517810\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080, pci bus id: 0000:0a:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7782773556\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14146744377013638174\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080, pci bus id: 0000:42:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8328103310966344648\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14079516633264224859\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(os.getcwd())\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.keras.backend.set_session(tf.Session(config=config))\n",
    "#config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "#sess = tf.Session(config=config) \n",
    "#keras.backend.set_session(sess)\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2099160, 51)\n",
      "Index(['Unnamed: 0', 'STUDENT_INDEX_MASTER', 'DATE', 'QUARTERS', 'AGE',\n",
      "       'ETHNICITY', 'SEX', 'NZDEP', 'TEST', 'HDL', 'LDL', 'TRI', 'TCL',\n",
      "       'TCHDL', 'STATINS', 'ATORVASTATIN', 'SIMVASTATIN',\n",
      "       'CHOLESTEROL_LOWERING', 'ARBs', 'ACE', 'ALPHA_BLOCKERS',\n",
      "       'BETA_BLOCKERS', 'BLOOD_PRESSURE_LOWERING', 'LIPID_LOWERING',\n",
      "       'DIURETICS', 'ANTIANGINAL_ANTIARRYTHMIC', 'CALCIUM_CHANNEL_BLOCKERS',\n",
      "       'ANTIHYPERTENSIVES', 'CARDIAC_GLYCOSIDES', 'NITRATES',\n",
      "       'SYMPATHOMIMETIC', 'OTHERS', 'ANTIPLATELETS', 'ANTICOAGULANTS',\n",
      "       'TRUE_HDL', 'TRUE_LDL', 'TRUE_TRI', 'TRUE_TCL', 'TRUE_TCHDL', 'SBP',\n",
      "       'SMOKING', 'EN_TCHDL', 'HX_DIABETES', 'FAMILY_HISTORY', 'DBP', 'HX_CVD',\n",
      "       'GEN_LIPID', 'RENAL', 'HX_DIABETES_YR', 'SBP2', 'DBP2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "VIEW_df = pd.read_csv('/home/whsu014/data/Cholesterol_PHH_360dayspy_with_true_values_01day_ethn_coded_PREDICT_Variables_sbp2dbp2.csv')\n",
    "print(VIEW_df.shape)\n",
    "print(VIEW_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2099160, 28)\n",
      "STUDENT_INDEX_MASTER          0\n",
      "DATE                          0\n",
      "QUARTERS                      0\n",
      "AGE                           0\n",
      "ETHNICITY                     0\n",
      "SEX                           0\n",
      "NZDEP                         0\n",
      "TEST                          0\n",
      "HDL                           0\n",
      "LDL                           0\n",
      "TRI                           0\n",
      "TCL                           0\n",
      "TCHDL                         0\n",
      "STATINS                       0\n",
      "ATORVASTATIN                  0\n",
      "SIMVASTATIN                   0\n",
      "SBP                           0\n",
      "DBP                           0\n",
      "SBP2                          0\n",
      "DBP2                        112\n",
      "SMOKING                      28\n",
      "EN_TCHDL                  23884\n",
      "HX_DIABETES                   0\n",
      "FAMILY_HISTORY                0\n",
      "HX_CVD                        0\n",
      "GEN_LIPID                     0\n",
      "RENAL                   1440208\n",
      "HX_DIABETES_YR          1417864\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "VIEW_sub_df = VIEW_df[['STUDENT_INDEX_MASTER', 'DATE', 'QUARTERS', \n",
    "                       'AGE', 'ETHNICITY', 'SEX', 'NZDEP', 'TEST', \n",
    "                       'HDL', 'LDL', 'TRI', 'TCL', 'TCHDL', 'STATINS', \n",
    "                       'ATORVASTATIN', 'SIMVASTATIN', 'SBP', 'DBP',\n",
    "                       'SBP2', 'DBP2', 'SMOKING', 'EN_TCHDL', \n",
    "                       'HX_DIABETES', 'FAMILY_HISTORY', 'HX_CVD', \n",
    "                       'GEN_LIPID', 'RENAL', 'HX_DIABETES_YR']]\n",
    "print(VIEW_sub_df.shape)\n",
    "print(VIEW_sub_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETHNICITY ['E' 'A' 'B' 'H' 'C' 'G' 'D' 'F']\n",
      "SEX ['F' 'M']\n",
      "NZDEP [4 1 3 5 2]\n",
      "SMOKING [ 0.  3.  1.  4.  5.  2. nan]\n",
      "HX_DIABETES [0. 1.]\n",
      "FAMILY_HISTORY [1. 0.]\n",
      "HX_CVD [0. 1.]\n",
      "GEN_LIPID [0. 1. 4. 2. 3.]\n",
      "RENAL [nan  0.  1.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "print(\"ETHNICITY\", VIEW_sub_df.ETHNICITY.unique())\n",
    "print(\"SEX\", VIEW_sub_df.SEX.unique())\n",
    "print(\"NZDEP\", VIEW_sub_df.NZDEP.unique())\n",
    "print(\"SMOKING\", VIEW_sub_df.SMOKING.unique())\n",
    "print(\"HX_DIABETES\", VIEW_sub_df.HX_DIABETES.unique())\n",
    "print(\"FAMILY_HISTORY\", VIEW_sub_df.FAMILY_HISTORY.unique())\n",
    "print(\"HX_CVD\", VIEW_sub_df.HX_CVD.unique())\n",
    "print(\"GEN_LIPID\", VIEW_sub_df.GEN_LIPID.unique())\n",
    "print(\"RENAL\", VIEW_sub_df.RENAL.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "1440208\n",
      "1417864\n"
     ]
    }
   ],
   "source": [
    "# find number of nan in SMOKING and RENAL\n",
    "print(VIEW_sub_df.SMOKING.isnull().sum())\n",
    "print(VIEW_sub_df.RENAL.isnull().sum())\n",
    "print(VIEW_sub_df.HX_DIABETES_YR.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2099160, 28)\n",
      "(2075136, 28)\n"
     ]
    }
   ],
   "source": [
    "##  drop DATE, RENAL and HX_DIABETES_YR for now \n",
    "VIEW_sub_df = VIEW_df[['STUDENT_INDEX_MASTER', 'DATE','QUARTERS', \n",
    "                       'AGE', 'ETHNICITY', 'SEX', 'NZDEP', 'TEST', \n",
    "                       'HDL', 'LDL', 'TRI', 'TCL', 'TCHDL', 'STATINS', \n",
    "                       'ATORVASTATIN', 'SIMVASTATIN', 'SBP', 'DBP',\n",
    "                       'SBP2', 'DBP2', 'SMOKING', 'EN_TCHDL', \n",
    "                       'HX_DIABETES', 'FAMILY_HISTORY', 'HX_CVD', \n",
    "                       'GEN_LIPID', 'RENAL', 'HX_DIABETES_YR']]\n",
    "# remove individuals that \n",
    "# contains nans\n",
    "print(VIEW_sub_df.shape)\n",
    "VIEW_sub_df = VIEW_sub_df[np.isfinite(VIEW_sub_df['DBP2'])]\n",
    "VIEW_sub_df = VIEW_sub_df[np.isfinite(VIEW_sub_df['SMOKING'])]\n",
    "VIEW_sub_df = VIEW_sub_df[np.isfinite(VIEW_sub_df['EN_TCHDL'])]\n",
    "print(VIEW_sub_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3597\n",
      "44184\n",
      "6486\n",
      "1234\n",
      "7236\n",
      "50\n",
      "2333\n",
      "8992\n"
     ]
    }
   ],
   "source": [
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'A'].shape[0]//28)\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'B'].shape[0]//28)\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'C'].shape[0]//28)\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'D'].shape[0]//28)\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'E'].shape[0]//28)\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'F'].shape[0]//28)\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'G'].shape[0]//28)\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'H'].shape[0]//28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3597\n",
      "44184\n",
      "6486\n",
      "3617\n",
      "7236\n",
      "8992\n"
     ]
    }
   ],
   "source": [
    "# Combine D, F and G\n",
    "VIEW_sub_A_df = VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'A']\n",
    "VIEW_sub_B_df = VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'B']\n",
    "VIEW_sub_C_df = VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'C']\n",
    "VIEW_sub_D_df = VIEW_sub_df[VIEW_sub_df.ETHNICITY.isin(['D', 'F', 'G'])] # MELAA, Other and Other_Asian\n",
    "VIEW_sub_E_df = VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'E']\n",
    "VIEW_sub_H_df = VIEW_sub_df[VIEW_sub_df.ETHNICITY == 'H']\n",
    "print(VIEW_sub_A_df.shape[0]//28)\n",
    "print(VIEW_sub_B_df.shape[0]//28)\n",
    "print(VIEW_sub_C_df.shape[0]//28)\n",
    "print(VIEW_sub_D_df.shape[0]//28)\n",
    "print(VIEW_sub_E_df.shape[0]//28)\n",
    "print(VIEW_sub_H_df.shape[0]//28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2075136, 28)\n",
      "(560000, 28)\n"
     ]
    }
   ],
   "source": [
    "# random sampling unique ID \n",
    "# to control ethn imbalance\n",
    "ethn_A_list = VIEW_sub_A_df.STUDENT_INDEX_MASTER.unique()\n",
    "ethn_B_list = VIEW_sub_B_df.STUDENT_INDEX_MASTER.unique()\n",
    "ethn_C_list = VIEW_sub_C_df.STUDENT_INDEX_MASTER.unique()\n",
    "ethn_D_list = VIEW_sub_D_df.STUDENT_INDEX_MASTER.unique()\n",
    "ethn_E_list = VIEW_sub_E_df.STUDENT_INDEX_MASTER.unique()\n",
    "ethn_H_list = VIEW_sub_H_df.STUDENT_INDEX_MASTER.unique()\n",
    "ethn_A_sample = list(np.random.choice(ethn_A_list, 3333, replace=False))\n",
    "ethn_B_sample = list(np.random.choice(ethn_B_list, 3335, replace=False)) # over sample Europeans\n",
    "ethn_C_sample = list(np.random.choice(ethn_C_list, 3333, replace=False)) # to make a data of \n",
    "ethn_D_sample = list(np.random.choice(ethn_D_list, 3333, replace=False)) # 20,000 individuals\n",
    "ethn_E_sample = list(np.random.choice(ethn_E_list, 3333, replace=False))\n",
    "ethn_H_sample = list(np.random.choice(ethn_H_list, 3333, replace=False))\n",
    "print(VIEW_sub_df.shape)\n",
    "ethn_sample = ethn_A_sample + ethn_B_sample + ethn_C_sample + \\\n",
    "              ethn_D_sample + ethn_E_sample + ethn_H_sample\n",
    "VIEW_sub_df = VIEW_sub_df[VIEW_sub_df.STUDENT_INDEX_MASTER.isin(ethn_sample)]\n",
    "print(VIEW_sub_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['STUDENT_INDEX_MASTER', 'DATE', 'QUARTERS', 'AGE', 'ETHNICITY', 'SEX',\n",
      "       'NZDEP', 'TEST', 'HDL', 'LDL', 'TRI', 'TCL', 'TCHDL', 'STATINS',\n",
      "       'ATORVASTATIN', 'SIMVASTATIN', 'SBP', 'DBP', 'SBP2', 'DBP2', 'SMOKING',\n",
      "       'EN_TCHDL', 'HX_DIABETES', 'FAMILY_HISTORY', 'HX_CVD', 'GEN_LIPID',\n",
      "       'RENAL', 'HX_DIABETES_YR'],\n",
      "      dtype='object')\n",
      "93324\n",
      "93380\n",
      "93324\n",
      "32116\n",
      "1260\n",
      "59948\n",
      "93324\n",
      "93324\n"
     ]
    }
   ],
   "source": [
    "print(VIEW_sub_df.columns) \n",
    "#check ETHN distribution\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == \"A\"].shape[0])\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == \"B\"].shape[0])\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == \"C\"].shape[0])\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == \"D\"].shape[0])\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == \"F\"].shape[0])\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == \"G\"].shape[0])\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == \"E\"].shape[0])\n",
    "print(VIEW_sub_df[VIEW_sub_df.ETHNICITY == \"H\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['STUDENT_INDEX_MASTER', 'DATE', 'QUARTERS', 'AGE', 'ETHNICITY', 'SEX',\n",
      "       'NZDEP', 'TEST', 'HDL', 'LDL', 'TRI', 'TCL', 'TCHDL', 'STATINS',\n",
      "       'ATORVASTATIN', 'SIMVASTATIN', 'SBP', 'DBP', 'SBP2', 'DBP2', 'SMOKING',\n",
      "       'EN_TCHDL', 'HX_DIABETES', 'FAMILY_HISTORY', 'HX_CVD', 'GEN_LIPID',\n",
      "       'RENAL', 'HX_DIABETES_YR'],\n",
      "      dtype='object')\n",
      "2009-07-01 00:00:00\n",
      "2007.0\n",
      "STUDENT_INDEX_MASTER         0\n",
      "DATE                         0\n",
      "QUARTERS                     0\n",
      "AGE                          0\n",
      "ETHNICITY                    0\n",
      "SEX                          0\n",
      "NZDEP                        0\n",
      "TEST                         0\n",
      "HDL                          0\n",
      "LDL                          0\n",
      "TRI                          0\n",
      "TCL                          0\n",
      "TCHDL                        0\n",
      "STATINS                      0\n",
      "ATORVASTATIN                 0\n",
      "SIMVASTATIN                  0\n",
      "SBP                          0\n",
      "DBP                          0\n",
      "SBP2                         0\n",
      "DBP2                         0\n",
      "SMOKING                      0\n",
      "EN_TCHDL                     0\n",
      "HX_DIABETES                  0\n",
      "FAMILY_HISTORY               0\n",
      "HX_CVD                       0\n",
      "GEN_LIPID                    0\n",
      "RENAL                   346136\n",
      "HX_DIABETES_YR          339108\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(VIEW_sub_df.columns)\n",
    "print(VIEW_sub_df.DATE.iloc[0])\n",
    "print(VIEW_sub_df.HX_DIABETES_YR.iloc[170])\n",
    "print(VIEW_sub_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode ID strings to numbers\n",
    "# create a dictionary to store \n",
    "# its mapping \n",
    "VIEW_sub_v = VIEW_sub_df.values\n",
    "ID_to_num = {}\n",
    "num = 0\n",
    "for i in range(len(VIEW_sub_v)):\n",
    "    if VIEW_sub_v[i, 0] not in ID_to_num:\n",
    "        ID_to_num[VIEW_sub_v[i, 0]] = num\n",
    "        num += 1\n",
    "    VIEW_sub_v[i, 0] = ID_to_num[VIEW_sub_v[i, 0]]\n",
    "# if ethn is , F or G change to D\n",
    "for i in range(len(VIEW_sub_v)):\n",
    "    if VIEW_sub_v[i, 4] in ['F', 'G']:\n",
    "        VIEW_sub_v[i, 4] = 'D'\n",
    "# change date into datetime object 26, 27\n",
    "for i in range(len(VIEW_sub_v)):\n",
    "    VIEW_sub_v[i, 1] = datetime.datetime.strptime(VIEW_sub_v[i, 1], '%Y-%m-%d %H:%M:%S')\n",
    "    # process RENAL values\n",
    "    # change RENAL [nan  0.  1.  2.  3.]\n",
    "    # to RENAL [0, 1, 2, 3, 4]\n",
    "    if np.isnan(VIEW_sub_v[i, 26]):\n",
    "        VIEW_sub_v[i, 26] = 0\n",
    "    else:\n",
    "        VIEW_sub_v[i, 26] = VIEW_sub_v[i, 26] + 1\n",
    "    # process HX_DIABETES_YR values\n",
    "    if np.isnan(VIEW_sub_v[i, 27]):\n",
    "        VIEW_sub_v[i, 27] = 0\n",
    "    else:\n",
    "        VIEW_sub_v[i, 27] = VIEW_sub_v[i, 1].year - VIEW_sub_v[i, 27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[59.0 60.0 60.0 60.0 60.0 61.0 61.0 61.0 61.0 62.0 62.0 62.0 62.0 63.0\n",
      " 63.0 63.0 63.0 64.0 64.0 64.0 64.0 64.0 65.0 65.0 65.0 65.0 66.0 66.0\n",
      " 54.0 55.0 55.0 55.0 55.0 56.0 56.0 56.0 56.0 57.0 57.0 57.0 57.0 58.0\n",
      " 58.0 58.0 58.0 59.0 59.0 59.0 59.0 60.0 60.0 60.0 60.0 61.0 61.0 61.0\n",
      " 63.0 63.0 64.0 64.0 64.0 64.0 65.0 65.0 65.0 65.0 66.0 66.0 66.0 66.0\n",
      " 67.0 67.0 67.0 67.0 68.0 68.0 68.0 68.0 69.0 69.0 69.0 69.0 70.0 70.0\n",
      " 49.0 49.0 49.0 49.0 50.0 50.0 50.0 50.0 51.0 51.0 51.0 51.0 52.0 52.0\n",
      " 52.0 52.0]\n",
      "ETHNICITY VALUES ['A' 'B' 'C' 'D' 'E' 'H']\n",
      "RENAL VALUES [0 1.0 2.0 3.0 4.0]\n",
      "HX_DIABETES_YR VALUES [0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0\n",
      " 17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0 30.0\n",
      " 31.0 32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0 40.0 41.0 42.0 43.0 44.0\n",
      " 45.0 46.0 47.0 49.0 51.0 53.0 56.0 57.0 58.0 63.0]\n",
      "832\n",
      "820\n",
      "897\n",
      "812\n",
      "827\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "print(pd.isnull(VIEW_sub_v).sum())\n",
    "print(VIEW_sub_v[0:100, 0])\n",
    "print(VIEW_sub_v[0:100, 3])\n",
    "print(\"ETHNICITY VALUES\", np.unique(VIEW_sub_v[:, 4]))\n",
    "print(\"RENAL VALUES\", np.unique(VIEW_sub_v[:, 26]))\n",
    "print(\"HX_DIABETES_YR VALUES\", np.unique(VIEW_sub_v[:, 27]))\n",
    "#check distribution of ETHNICITY in the first 5000 individuals\n",
    "VIEW_sub_sub = VIEW_sub_v[0:(5000*28), :]\n",
    "print(VIEW_sub_sub[VIEW_sub_sub[:, 4]== \"A\"].shape[0]//28)\n",
    "print(VIEW_sub_sub[VIEW_sub_sub[:, 4]== \"B\"].shape[0]//28)\n",
    "print(VIEW_sub_sub[VIEW_sub_sub[:, 4]== \"C\"].shape[0]//28)\n",
    "print(VIEW_sub_sub[VIEW_sub_sub[:, 4]== \"D\"].shape[0]//28)\n",
    "print(VIEW_sub_sub[VIEW_sub_sub[:, 4]== \"E\"].shape[0]//28)\n",
    "print(VIEW_sub_sub[VIEW_sub_sub[:, 4]== \"H\"].shape[0]//28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'STUDENT_INDEX_MASTER', 'DATE', 'QUARTERS', 'AGE', 'ETHNICITY', 'SEX',\n",
    "       'NZDEP', 'TEST', 'HDL', 'LDL', 'TRI', 'TCL', 'TCHDL', 'STATINS',\n",
    "       'ATORVASTATIN', 'SIMVASTATIN', 'SBP', 'DBP', 'SBP2', 'DBP2', 'SMOKING',\n",
    "       'EN_TCHDL', 'HX_DIABETES', 'FAMILY_HISTORY', 'HX_CVD', 'GEN_LIPID',\n",
    "       'RENAL', 'HX_DIABETES_YR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 32)\n"
     ]
    }
   ],
   "source": [
    "# one hot encode categorical variables\n",
    "for i in range(len(VIEW_sub_v)):\n",
    "    if VIEW_sub_v[i, 5] == 'M':\n",
    "        VIEW_sub_v[i, 5] = 0\n",
    "    else:\n",
    "        VIEW_sub_v[i, 5] = 1\n",
    "        \n",
    "ethn_v = np.zeros((len(VIEW_sub_v), 6))\n",
    "ethn_idx = {'A': 0,\n",
    "            'B': 1,\n",
    "            'C': 2,\n",
    "            'D': 3,\n",
    "            'E': 4,\n",
    "            'H': 5}\n",
    "\n",
    "for i in range(len(VIEW_sub_v)):\n",
    "    ethn_v[i, ethn_idx[VIEW_sub_v[i, 4]]] = 1\n",
    "\n",
    "# reorganise columns  \n",
    "# STUDENT_INDEX_MASTER, QUARTERS, AGE, SEX, NZDEP, \n",
    "# A, B, C, D, E, H\n",
    "# SBP, DBP, SBP2, DBP2,\n",
    "# SMOKING, EN_TCHDL, HX_DIABETES, FAMILY_HISTORY, HX_CVD, \n",
    "# GEN_LIPID, RENAL, HX_DIABETES_YR\n",
    "# STATINS, ATORVASTATIN, SIMVASTATIN, \n",
    "# TEST, HDL, LDL, TRI, TCL, TCHDL\n",
    "VIEW_sub_v1 = VIEW_sub_v[:, [0,2,3,5,6]] # Demographic variables + QUARTERS\n",
    "VIEW_sub_v2 = VIEW_sub_v[:, [13,14,15]] # STATINS  PHH\n",
    "VIEW_sub_v3 = VIEW_sub_v[:, [16,17,18,19,20,21,22,23,24,25,26,27]] # PREDICT variables\n",
    "VIEW_sub_v4 = VIEW_sub_v[:, [7,8,9,10,11,12]] # CHOLESTEROLS TESTSAFE\n",
    "VIEW_1hot_encoded_v = np.append(VIEW_sub_v1, ethn_v, axis= 1)\n",
    "VIEW_1hot_encoded_v = np.append(VIEW_1hot_encoded_v, VIEW_sub_v3, \\\n",
    "                                axis=1)\n",
    "VIEW_1hot_encoded_v = np.append(VIEW_1hot_encoded_v, VIEW_sub_v2, \\\n",
    "                                axis=1)\n",
    "VIEW_1hot_encoded_v = np.append(VIEW_1hot_encoded_v, VIEW_sub_v4, \\\n",
    "                                axis=1)\n",
    "print(VIEW_1hot_encoded_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "TEST VALUES [-0.71651295  1.39564819]\n",
      "Value indicating test 1.3956481893756667\n"
     ]
    }
   ],
   "source": [
    "# normalise vectors to values between 0 and 1\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#scaled_train = scaler.fit_transform(VIEW_1hot_encoded_v)\n",
    "#decided to use standardise instead of normalise\n",
    "print(np.unique(VIEW_1hot_encoded_v[:, -6]))\n",
    "scaler = StandardScaler()\n",
    "scaled_train = scaler.fit_transform(VIEW_1hot_encoded_v)\n",
    "print(\"TEST VALUES\", np.unique(scaled_train[:, -6]))\n",
    "print(\"Value indicating test\", max(np.unique(scaled_train[:, -6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.0000e+00, 1.5200e+02, 2.3300e+03, 1.1758e+04, 2.7160e+04,\n",
       "        4.5367e+04, 5.9666e+04, 6.7795e+04, 7.1247e+04, 6.2932e+04,\n",
       "        5.5210e+04, 4.4681e+04, 3.5214e+04, 2.6462e+04, 1.7798e+04,\n",
       "        1.1776e+04, 7.7190e+03, 5.0030e+03, 2.9330e+03, 1.6420e+03,\n",
       "        1.0420e+03, 7.4100e+02, 4.5100e+02, 2.5000e+02, 1.7200e+02,\n",
       "        1.1800e+02, 1.0000e+02, 6.7000e+01, 3.2000e+01, 2.8000e+01,\n",
       "        2.4000e+01, 1.9000e+01, 8.0000e+00, 1.1000e+01, 5.0000e+00,\n",
       "        3.0000e+00, 6.0000e+00, 3.0000e+00, 1.0000e+00, 4.0000e+00,\n",
       "        5.0000e+00, 6.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        2.0000e+00, 3.0000e+00, 7.0000e+00, 4.0000e+00, 4.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 1.0000e+00, 2.0000e+00, 2.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        3.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 2.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        2.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([-2.75253103, -2.45756054, -2.16259006, -1.86761958, -1.5726491 ,\n",
       "        -1.27767862, -0.98270813, -0.68773765, -0.39276717, -0.09779669,\n",
       "         0.1971738 ,  0.49214428,  0.78711476,  1.08208524,  1.37705573,\n",
       "         1.67202621,  1.96699669,  2.26196717,  2.55693765,  2.85190814,\n",
       "         3.14687862,  3.4418491 ,  3.73681958,  4.03179007,  4.32676055,\n",
       "         4.62173103,  4.91670151,  5.211672  ,  5.50664248,  5.80161296,\n",
       "         6.09658344,  6.39155393,  6.68652441,  6.98149489,  7.27646537,\n",
       "         7.57143585,  7.86640634,  8.16137682,  8.4563473 ,  8.75131778,\n",
       "         9.04628827,  9.34125875,  9.63622923,  9.93119971, 10.2261702 ,\n",
       "        10.52114068, 10.81611116, 11.11108164, 11.40605212, 11.70102261,\n",
       "        11.99599309, 12.29096357, 12.58593405, 12.88090454, 13.17587502,\n",
       "        13.4708455 , 13.76581598, 14.06078647, 14.35575695, 14.65072743,\n",
       "        14.94569791, 15.2406684 , 15.53563888, 15.83060936, 16.12557984,\n",
       "        16.42055032, 16.71552081, 17.01049129, 17.30546177, 17.60043225,\n",
       "        17.89540274, 18.19037322, 18.4853437 , 18.78031418, 19.07528467,\n",
       "        19.37025515, 19.66522563, 19.96019611, 20.25516659, 20.55013708,\n",
       "        20.84510756, 21.14007804, 21.43504852, 21.73001901, 22.02498949,\n",
       "        22.31995997, 22.61493045, 22.90990094, 23.20487142, 23.4998419 ,\n",
       "        23.79481238, 24.08978287, 24.38475335, 24.67972383, 24.97469431,\n",
       "        25.26966479, 25.56463528, 25.85960576, 26.15457624, 26.44954672,\n",
       "        26.74451721]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pyplot.hist(VIEW_1hot_encoded_v[:, -1], bins= 100)\n",
    "pyplot.hist(scaled_train[:, -1], bins= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 32)\n",
      "(140000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Create test data array\n",
    "# ensuring test does not \n",
    "# look foreward beyond \n",
    "# last test in the initial\n",
    "# 8 quarters\n",
    "number_of_inds = 5000#VIEW_1hot_encoded_v.shape[0]//28\n",
    "start = 0\n",
    "end = 28\n",
    "value_indicating_test = max(np.unique(scaled_train[:, -6]))\n",
    "value_indicating_no_test = min(np.unique(scaled_train[:, -6]))\n",
    "scaled_test = np.empty([0, 32])\n",
    "for i in range(number_of_inds):\n",
    "    ind_v = copy.deepcopy(scaled_train[start:end, :])\n",
    "    test_list = list(ind_v[0:8, -6])\n",
    "    quarters_v = copy.deepcopy(ind_v[:, 1])\n",
    "    age_v = copy.deepcopy(ind_v[:, 2])\n",
    "    idx = len(test_list) - 1 - \\\n",
    "          test_list[::-1].index(value_indicating_test)\n",
    "    last_v = copy.deepcopy(ind_v[idx, -6:])\n",
    "    last_v[0] = value_indicating_no_test #this might be the wrong assumption\n",
    "    for j in range((idx+1),28):\n",
    "        ind_v[j, -6:] = last_v\n",
    "        #ind_v[j, -6] = 0  # set TEST to 0\n",
    "    ind_v[:, 1] = quarters_v\n",
    "    ind_v[:, 2] = age_v\n",
    "    scaled_test = np.append(scaled_test, \\\n",
    "                            ind_v, axis = 0)\n",
    "    start = end\n",
    "    end += 28\n",
    "print(scaled_train.shape)\n",
    "print(scaled_test.shape)\n",
    "#VIEW_1hot_encoded_v = VIEW_1hot_encoded_v[0:(number_of_inds*28), :]\n",
    "#print(VIEW_1hot_encoded_v.shape)\n",
    "#print(VIEW_1hot_encoded_test_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84 -0.38 -0.64 -0.72 -0.47  0.48  0.3   0.43  0.71]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.33  0.62  0.07  0.46  0.54]\n",
      " [-0.84 -0.38 -0.64  1.39 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -0.3   0.7   0.06  0.52  0.56]\n",
      " [-0.84 -0.38 -0.64 -0.72 -1.09  0.21  0.39  0.06  1.33]]\n"
     ]
    }
   ],
   "source": [
    "#inspect inds\n",
    "ind = 1\n",
    "print(np.around(scaled_test[(ind*28):((ind+1)*28)+1, -9:], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 32)\n",
      "(560000, 26)\n",
      "(560000, 32)\n",
      "(560000, 26)\n"
     ]
    }
   ],
   "source": [
    "#STUDENT_INDEX_MASTER, QUARTERS, AGE, SEX, NZDEP, \n",
    "# A, B, C, D, E, H (10)\n",
    "# SBP, DBP, SBP2, DBP2,\n",
    "# SMOKING, EN_TCHDL, HX_DIABETES, FAMILY_HISTORY, HX_CVD, \n",
    "# GEN_LIPID, RENAL, HX_DIABETES_YR\n",
    "# STATINS, ATORVASTATIN, SIMVASTATIN, \n",
    "# TEST, HDL, LDL, TRI, TCL, TCHDL\n",
    "#VIEW_1hot_encoded_non_sparse_v = VIEW_1hot_encoded_v[:, [0,1,2,3,4,5,6,\\\n",
    "#                                                         7,8,9,10,11,12,\\\n",
    "#                                                         13,14,15,16,23,\\\n",
    "#                                                         24,25,26,27,28,\\\n",
    "#                                                         29,30,31]]\n",
    "#VIEW_1hot_encoded_test_non_sparse_v = VIEW_1hot_encoded_test_v[:, [0,1,2,3,4,5,6,\\\n",
    "#                                                         7,8,9,10,11,12,\\\n",
    "#                                                         13,14,15,16,23,\\\n",
    "#                                                         24,25,26,27,28,\\\n",
    "#                                                         29,30,31]]\n",
    "#print(VIEW_1hot_encoded_v.shape)\n",
    "#print(VIEW_1hot_encoded_non_sparse_v.shape)\n",
    "#print(VIEW_1hot_encoded_test_v.shape)\n",
    "#print(VIEW_1hot_encoded_test_non_sparse_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# set up data for training with generator\n",
    "# build x and y list\n",
    "###############################################\n",
    "\n",
    "def generate_x_y(scaled, number_of_inds, folds):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    x = np.empty([0, 32])\n",
    "    ground_truth = np.empty([0, 32])\n",
    "    ##################\n",
    "    entry_start = 0\n",
    "    len_list = [0]\n",
    "    len_list_x = [0]\n",
    "    len_list_y = [0]\n",
    "    id_count = 0\n",
    "    ##################\n",
    "    #number_of_inds = len(scaled)//28\n",
    "    for i in range(number_of_inds):\n",
    "        start = i*28\n",
    "        end = (i+1)*28\n",
    "        ind_data = scaled[start:end, :]\n",
    "        ##################################\n",
    "        # create ground truth for forecast\n",
    "        ##################################\n",
    "        id_count += 1\n",
    "        ind_x = ind_data[:-1, :-9]\n",
    "        ind_y = ind_data[1:, -9:]\n",
    "        ind_xy = np.append(ind_x, ind_y, axis=1)\n",
    "        ground_truth = np.append(ground_truth, ind_xy, axis=0)\n",
    "        ##################################\n",
    "        # create data for 1 step forecast\n",
    "        ##################################\n",
    "        for i in range(len(ind_data)-1):\n",
    "            x_list.append(ind_data[:(i+1), :])\n",
    "            y_list.append(ind_data[(i+1), -9:])\n",
    "            if i == len(ind_data)-2:\n",
    "                x = np.append(x, x_list[-1], axis=0)\n",
    "                n_rows = x_list[-1].shape[0]\n",
    "                entry_start += n_rows\n",
    "                len_list.append(entry_start)\n",
    "        # store the indices to fold\n",
    "        # changed from 10 fold to 2 fold \n",
    "        # due to amount of data and fair \n",
    "        # comparison with depmix\n",
    "        if id_count % (number_of_inds//folds) == 0 and \\\n",
    "            id_count != 0:\n",
    "                len_list_x.append(len(x_list))\n",
    "                len_list_y.append(len(y_list))\n",
    "    return x_list, y_list, len_list_x, len_list_y, len_list, x, ground_truth\n",
    "x_train_list, y_train_list, \\\n",
    "len_list_x_train, len_list_y_train, \\\n",
    "len_list_train, x_train, ground_truth_train = generate_x_y(scaled_train, number_of_inds, 2)\n",
    "\n",
    "x_test_list, y_test_list, \\\n",
    "len_list_x_test, len_list_y_test, \\\n",
    "len_list_test, x_test, ground_truth_test = generate_x_y(scaled_test, number_of_inds, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "2500\n",
      "0\n",
      "67500\n",
      "x_train[start:end, :-9] shape (67500, 23)\n",
      "yhat shape (54000, 9)\n"
     ]
    }
   ],
   "source": [
    "start_idx = 0\n",
    "size = len(len_list_train)//2\n",
    "print(len(len_list_train))\n",
    "print(size)\n",
    "start = len_list_train[start_idx]\n",
    "end = len_list_train[start_idx+size]\n",
    "print(start)\n",
    "print(end)\n",
    "print(\"x_train[start:end, :-9] shape\", x_train[start:end, :-9].shape)\n",
    "print(\"yhat shape\", yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "Epoch 1/50\n",
      " - 323s - loss: 0.3542 - val_loss: 0.2926\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29261, saving model to weights0.hdf5\n",
      "Epoch 2/50\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "###########################################\n",
    "# two fold cross validatioin\n",
    "###########################################\n",
    "\n",
    "# keras generators need to be \n",
    "# infinitely iterable\n",
    "def train_generator(x_list, y_list):\n",
    "    # 0.1 validatioin split\n",
    "    train_length = (len(x_list)//10)*9\n",
    "    while True:\n",
    "        for i in range(train_length):\n",
    "            train_x = np.array([x_list[i]])\n",
    "            train_y = np.array([y_list[i]])\n",
    "            yield train_x, train_y\n",
    "            \n",
    "def val_generator(x_list, y_list):\n",
    "#    # 0.1 validation split\n",
    "    val_length = len(x_list)//10\n",
    "    while True:\n",
    "        for i in range(-val_length, 0, 1):\n",
    "            val_x = np.array([x_list[i]])\n",
    "            val_y = np.array([y_list[i]])\n",
    "            yield val_x, val_y\n",
    "            \n",
    "###########################################\n",
    "# use keras.utils.Sequence for \n",
    "# thread-safe multiprocess\n",
    "###########################################\n",
    "start_idx = 0\n",
    "size = len(len_list_train)//2 #changed from 10\n",
    "\n",
    "rmse_lstm_list = []\n",
    "rmse_naive_list = []\n",
    "\n",
    "complete_inv_y = np.empty([0, 32])\n",
    "complete_inv_yhat = np.empty([0, 32])\n",
    "complete_inv_naive = np.empty([0, 32])\n",
    "\n",
    "start_outside = time.time()\n",
    "\n",
    "for f in range(2):  #2\n",
    "    print(\"Fold: \", f+1)\n",
    "    ##################################\n",
    "    # forecast using LSTM\n",
    "    ##################################\n",
    "    lstm_start = len_list_x_train[f]\n",
    "    lstm_end = len_list_x_train[f+1]\n",
    "    \n",
    "\n",
    "    train1_x = x_train_list[:lstm_start]\n",
    "    train2_x = x_train_list[lstm_end:]\n",
    "    train_x = train1_x + train2_x\n",
    "    \n",
    "    test_x = x_test_list[lstm_start:lstm_end]\n",
    "    \n",
    "    train1_y = y_train_list[:lstm_start]\n",
    "    train2_y = y_train_list[lstm_end:]\n",
    "    train_y = train1_y + train2_y\n",
    "    \n",
    "    test_y = y_test_list[lstm_start:lstm_end]\n",
    "    \n",
    "    # time training\n",
    "    start_time = time.time()\n",
    "    # design network\n",
    "    #with tf.device('/device:GPU:0'):\n",
    "    with tf.Session(config = tf.ConfigProto(log_device_placement = True)):        \n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(256, return_sequences=True,\n",
    "                        input_shape=(None, 32)))\n",
    "        model.add(CuDNNLSTM(256, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(256))\n",
    "        model.add(Dense(9))\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        weight = \"weights\" + str(f) + \".hdf5\"\n",
    "        checkpointer = ModelCheckpoint(filepath=weight,\n",
    "                                    monitor='val_loss', verbose=1,\n",
    "                                    save_best_only=True)\n",
    "        history = model.fit_generator(generator=train_generator(train_x, \n",
    "                                                                train_y), \n",
    "                                        steps_per_epoch=(len(train_x)//10)*9, \n",
    "                                        epochs=50, \n",
    "                                        validation_data=val_generator(train_x, \n",
    "                                                                    train_y), \n",
    "                                        validation_steps=len(train_x)//10, \n",
    "                                        callbacks=[checkpointer], \n",
    "                                        verbose=2, shuffle=False)\n",
    "\n",
    "    # plot history\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Training LSTM: \", end_time - start_time)\n",
    "    #########################################\n",
    "    # LSTM prediction\n",
    "    #########################################\n",
    "    model.load_weights(weight)\n",
    "    yhat = np.empty((0, 9))\n",
    "    forecast = np.empty((0, 9))\n",
    "    for i in range(len(test_x)):\n",
    "        #yhat_pred = model.predict(np.array([test_x[i]]))\n",
    "        #print(\"yhat_pred.shape: \", yhat_pred.shape)\n",
    "        #yhat = np.append(yhat, yhat_pred, axis=0)\n",
    "        if len(test_x[i]) < 8:\n",
    "            yhat_pred = model.predict(np.array([test_x[i]]))\n",
    "        elif len(test_x[i]) == 8:\n",
    "            yhat_pred = model.predict(np.array([test_x[i]]))\n",
    "            #print(\"forecast shape\", forecast.shape)\n",
    "            #print(\"yhat_pred shape\", yhat_pred.shape)\n",
    "            forecast = np.append(forecast, yhat_pred, axis=0)\n",
    "        elif len(test_x[i]) > 8:\n",
    "            adjusted_y = np.append(test_x[i][:-len(forecast), -9:], forecast, axis=0)\n",
    "            test_x[i] = np.append(test_x[i][:, :-9], adjusted_y, axis=1)\n",
    "            yhat_pred = model.predict(np.array([test_x[i]]))\n",
    "            #print(\"forecast shape\", forecast.shape)\n",
    "            #print(\"yhat_pred shape\", yhat_pred.shape)\n",
    "            forecast = np.append(forecast, yhat_pred, axis=0)    \n",
    "            if len(test_x[i]) == 27:\n",
    "                forecast = np.empty((0, 9))\n",
    "        #print(\"yhat shape\", yhat.shape)\n",
    "        #print(\"yhat_pred shape\", yhat_pred.shape)\n",
    "        yhat = np.append(yhat, yhat_pred, axis=0)\n",
    "    start = len_list_train[start_idx]\n",
    "    end = len_list_train[start_idx+size]\n",
    "    #print(\"x_train[start:end, :-9] shape\", x_train[start:end, :-9].shape)\n",
    "    #print(\"yhat shape\", yhat.shape)\n",
    "    xyhat = np.append(x_train[start:end, :-9], yhat, axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(xyhat)\n",
    "    inv_y = scaler.inverse_transform(ground_truth_train[start:end, :])\n",
    "    if np.array_equal(inv_y[:, :-9], inv_yhat[:, :-9]):\n",
    "        print(\"inv_y x and inv_yhat x are the same\")\n",
    "    else:\n",
    "        print(\"inv_y x and inv_yhat x are not the same\")\n",
    "    end_time = time.time()\n",
    "    print(\"Training and Testing LSTM: \", end_time - start_time)\n",
    "    #########################################################\n",
    "    # forecasting using Vector Autoregression (VAR) and Naive\n",
    "    #########################################################\n",
    "    rmse_lstm = math.sqrt(mean_squared_error(inv_y[:, -9:], inv_yhat[:, -9:]))\n",
    "    rmse_lstm_list.append(rmse_lstm)\n",
    "    \n",
    "    complete_inv_y = np.append(complete_inv_y, inv_y, axis=0)\n",
    "    complete_inv_yhat = np.append(complete_inv_yhat, inv_yhat, axis=0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"End of fold \"+str(f)+\":\", end_time - start_time)\n",
    "    start_idx += size\n",
    "print(\"rmse_lstm_list\")\n",
    "print(rmse_lstm_list)\n",
    "end_outside = time.time()\n",
    "print(\"Entire process took:\", end_outside - start_outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_inv_y_df = pd.DataFrame(complete_inv_y)\n",
    "complete_inv_y_df.columns = ['STUDENT_INDEX_MASTER', 'QUARTERS',\n",
    "                             'AGE', 'SEX', 'NZDEP', 'ETHN_A', \n",
    "                             'ETHN_B', 'ETHN_C', 'ETHN_D', \n",
    "                             'ETHN_E', 'ETHN_H',# (10)\n",
    "                             'SBP', 'DBP', 'SBP2', 'DBP2',\n",
    "                             'SMOKING', 'EN_TCHDL', 'HX_DIABETES',\n",
    "                             'FAMILY_HISTORY', 'HX_CVD', 'GEN_LIPID',\n",
    "                             'RENAL', 'HX_DIABETES_YR', 'STATINS',\n",
    "                             'ATORVASTATIN', 'SIMVASTATIN', \n",
    "                             'TEST', 'HDL', 'LDL', 'TRI', 'TCL', \n",
    "                             'TCHDL']\n",
    "complete_inv_y_df.to_csv('/home/whsu014/data/LSTM_experiment_train2500inds_ID_y.csv', sep=\",\")\n",
    "complete_inv_yhat_df = pd.DataFrame(complete_inv_yhat)\n",
    "complete_inv_yhat_df.columns = ['STUDENT_INDEX_MASTER', 'QUARTERS',\n",
    "                             'AGE', 'SEX', 'NZDEP', 'ETHN_A', \n",
    "                             'ETHN_B', 'ETHN_C', 'ETHN_D', \n",
    "                             'ETHN_E', 'ETHN_H',# (10)\n",
    "                             'SBP', 'DBP', 'SBP2', 'DBP2',\n",
    "                             'SMOKING', 'EN_TCHDL', 'HX_DIABETES',\n",
    "                             'FAMILY_HISTORY', 'HX_CVD', 'GEN_LIPID',\n",
    "                             'RENAL', 'HX_DIABETES_YR', 'STATINS',\n",
    "                             'ATORVASTATIN', 'SIMVASTATIN', \n",
    "                             'TEST', 'HDL', 'LDL', 'TRI', 'TCL', \n",
    "                             'TCHDL']\n",
    "complete_inv_yhat_df.to_csv('/home/whsu014/data/LSTM_experiment_train2500inds_ID_yhat.csv', sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'STUDENT_INDEX_MASTER', 'QUARTERS', 'AGE', 'SEX', 'NZDEP',\n",
      "       'ETHN_A', 'ETHN_B', 'ETHN_C', 'ETHN_D', 'ETHN_E', 'ETHN_H', 'SBP',\n",
      "       'DBP', 'SBP2', 'DBP2', 'SMOKING', 'EN_TCHDL', 'HX_DIABETES',\n",
      "       'FAMILY_HISTORY', 'HX_CVD', 'GEN_LIPID', 'RENAL', 'HX_DIABETES_YR',\n",
      "       'STATINS', 'ATORVASTATIN', 'SIMVASTATIN', 'TEST', 'HDL', 'LDL', 'TRI',\n",
      "       'TCL', 'TCHDL'],\n",
      "      dtype='object')\n",
      "(135000, 33)\n",
      "(135000, 32)\n",
      "(135000, 32)\n"
     ]
    }
   ],
   "source": [
    "complete_inv_y1_df = pd.read_csv('/home/whsu014/data/LSTM_experiment_train2500inds_y.csv')\n",
    "complete_inv_yhat1_df = pd.read_csv('/home/whsu014/data/LSTM_experiment_train2500inds_yhat.csv')\n",
    "print(complete_inv_y1_df.columns)\n",
    "print(complete_inv_y1_df.shape)\n",
    "complete_inv_y = complete_inv_y1_df.values[:, 1:]\n",
    "complete_inv_yhat = complete_inv_yhat1_df.values[:, 1:]\n",
    "print(complete_inv_y.shape)\n",
    "print(complete_inv_yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135000, 32)\n",
      "(135000, 32)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "['STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz'\n",
      " 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz'\n",
      " 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz'\n",
      " 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz'\n",
      " 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz']\n",
      "['STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz'\n",
      " 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz'\n",
      " 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz'\n",
      " 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz'\n",
      " 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz' 'STGeicKXNtxz']\n"
     ]
    }
   ],
   "source": [
    "print(complete_inv_y.shape)\n",
    "print(complete_inv_yhat.shape)\n",
    "print(complete_inv_y[0:20, 0])\n",
    "num_to_ID = {v: k for k, v in ID_to_num.items()}\n",
    "complete_inv_y = np.array(complete_inv_y, dtype= np.object)\n",
    "complete_inv_yhat = np.array(complete_inv_yhat, dtype= np.object)\n",
    "for i in range(len(complete_inv_y)):\n",
    "    complete_inv_y[i, 0] = num_to_ID[round(complete_inv_y[i, 0])]\n",
    "    complete_inv_yhat[i, 0] = num_to_ID[round(complete_inv_yhat[i, 0])]\n",
    "print(complete_inv_y[0:20, 0])\n",
    "print(complete_inv_yhat[0:20, 0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'STUDENT_INDEX_MASTER', 'DATE', 'QUARTERS', 'AGE',\n",
      "       'ETHNICITY', 'SEX', 'NZDEP', 'TEST', 'HDL', 'LDL', 'TRI', 'TCL',\n",
      "       'TCHDL', 'STATINS', 'ATORVASTATIN', 'SIMVASTATIN',\n",
      "       'CHOLESTEROL_LOWERING', 'ARBs', 'ACE', 'ALPHA_BLOCKERS',\n",
      "       'BETA_BLOCKERS', 'BLOOD_PRESSURE_LOWERING', 'LIPID_LOWERING',\n",
      "       'DIURETICS', 'ANTIANGINAL_ANTIARRYTHMIC', 'CALCIUM_CHANNEL_BLOCKERS',\n",
      "       'ANTIHYPERTENSIVES', 'CARDIAC_GLYCOSIDES', 'NITRATES',\n",
      "       'SYMPATHOMIMETIC', 'OTHERS', 'ANTIPLATELETS', 'ANTICOAGULANTS',\n",
      "       'TRUE_HDL', 'TRUE_LDL', 'TRUE_TRI', 'TRUE_TCL', 'TRUE_TCHDL', 'SBP',\n",
      "       'SMOKING', 'EN_TCHDL', 'HX_DIABETES', 'FAMILY_HISTORY', 'DBP', 'HX_CVD',\n",
      "       'GEN_LIPID', 'RENAL', 'HX_DIABETES_YR', 'SBP2', 'DBP2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(VIEW_df.columns)\n",
    "VIEW_v = VIEW_df.values[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "(2099160, 16)\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(complete_inv_y[:, 0])))\n",
    "print(len(np.unique(complete_inv_yhat[:, 0])))\n",
    "VIEW_ground_truth_v = VIEW_df.iloc[:, [1,3,8,9,10,11,12,13,14,15,16,34,35,36,37,38]].values\n",
    "print(VIEW_ground_truth_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 32)\n",
      "(27, 32)\n",
      "(28, 16)\n",
      "[[0.   0.   0.   0.   1.83 3.74 0.89 6.   3.25]\n",
      " [0.   0.   0.   0.   1.82 3.82 0.86 6.06 3.3 ]\n",
      " [0.   0.   0.   0.   1.81 3.91 0.83 6.13 3.35]\n",
      " [0.   0.   0.   0.   1.8  4.   0.8  6.2  3.4 ]\n",
      " [0.   0.   0.   1.   1.82 3.98 0.9  6.22 3.38]\n",
      " [0.   0.   0.   0.   1.84 3.95 1.   6.25 3.35]\n",
      " [0.   0.   0.   0.   1.87 3.92 1.1  6.28 3.32]\n",
      " [0.   0.   0.   1.   1.89 3.9  1.2  6.3  3.3 ]\n",
      " [0.   0.   0.   0.   1.87 3.82 1.22 6.21 3.29]\n",
      " [0.   0.   0.   0.   1.85 3.75 1.25 6.13 3.28]\n",
      " [0.   0.   0.   0.   1.83 3.67 1.27 6.05 3.28]\n",
      " [0.   0.   0.   0.   1.81 3.59 1.29 5.96 3.27]\n",
      " [0.   0.   0.   0.   1.79 3.51 1.32 5.88 3.26]\n",
      " [0.   0.   0.   0.   1.77 3.44 1.34 5.79 3.25]\n",
      " [0.   0.   0.   0.   1.75 3.36 1.36 5.71 3.25]\n",
      " [0.   0.   0.   0.   1.73 3.28 1.38 5.62 3.24]\n",
      " [0.   0.   0.   0.   1.71 3.21 1.41 5.54 3.23]\n",
      " [0.   0.   0.   0.   1.69 3.13 1.43 5.45 3.22]\n",
      " [0.   0.   0.   0.   1.67 3.05 1.45 5.37 3.22]\n",
      " [0.   0.   0.   0.   1.65 2.98 1.48 5.28 3.21]\n",
      " [0.   0.   0.   1.   1.63 2.9  1.5  5.2  3.2 ]\n",
      " [0.   0.   0.   0.   1.64 3.11 1.52 5.43 3.31]\n",
      " [0.   0.   0.   0.   1.66 3.31 1.55 5.65 3.43]\n",
      " [0.   0.   0.   0.   1.67 3.51 1.57 5.88 3.54]\n",
      " [0.   0.   0.   0.   1.68 3.71 1.59 6.1  3.65]\n",
      " [0.   0.   0.   1.   1.69 3.8  1.6  6.2  3.7 ]\n",
      " [0.   0.   0.   0.   1.69 3.8  1.6  6.2  3.7 ]]\n",
      "[[0.   1.84 3.65 0.92 5.93 3.21]\n",
      " [0.   1.83 3.74 0.89 6.   3.25]\n",
      " [0.   1.82 3.82 0.86 6.06 3.3 ]\n",
      " [0.   1.81 3.91 0.83 6.13 3.35]\n",
      " [0.   1.8  4.   0.8  6.2  3.4 ]\n",
      " [1.   1.82 3.98 0.9  6.22 3.38]\n",
      " [0.   1.84 3.95 1.   6.25 3.35]\n",
      " [0.   1.87 3.92 1.1  6.28 3.32]\n",
      " [1.   1.89 3.9  1.2  6.3  3.3 ]\n",
      " [0.   1.87 3.82 1.22 6.21 3.29]\n",
      " [0.   1.85 3.75 1.25 6.13 3.28]\n",
      " [0.   1.83 3.67 1.27 6.05 3.28]\n",
      " [0.   1.81 3.59 1.29 5.96 3.27]\n",
      " [0.   1.79 3.51 1.32 5.88 3.26]\n",
      " [0.   1.77 3.44 1.34 5.79 3.25]\n",
      " [0.   1.75 3.36 1.36 5.71 3.25]\n",
      " [0.   1.73 3.28 1.38 5.62 3.24]\n",
      " [0.   1.71 3.21 1.41 5.54 3.23]\n",
      " [0.   1.69 3.13 1.43 5.45 3.22]\n",
      " [0.   1.67 3.05 1.45 5.37 3.22]\n",
      " [0.   1.65 2.98 1.48 5.28 3.21]\n",
      " [1.   1.63 2.9  1.5  5.2  3.2 ]\n",
      " [0.   1.64 3.11 1.52 5.43 3.31]\n",
      " [0.   1.66 3.31 1.55 5.65 3.43]\n",
      " [0.   1.67 3.51 1.57 5.88 3.54]\n",
      " [0.   1.68 3.71 1.59 6.1  3.65]\n",
      " [1.   1.69 3.8  1.6  6.2  3.7 ]\n",
      " [0.   1.69 3.8  1.6  6.2  3.7 ]]\n",
      "0.03918170928955078\n"
     ]
    }
   ],
   "source": [
    "id_list = np.unique(complete_inv_y[:, 0])\n",
    "start_t = time.time()\n",
    "for i in range(1):\n",
    "    \n",
    "    #ind_y = complete_inv_y[(i*27):((i+1)*27), ]\n",
    "    #ind_yhat = complete_inv_yhat[(i*27):((i+1)*27), ]\n",
    "    #ind_id = ind_y[0,0]\n",
    "    #ind_view = VIEW_sub_v[VIEW_sub_v[:, 0] == ind_id]\n",
    "    ind_id = id_list[i]\n",
    "    ind_y = complete_inv_y[complete_inv_y[:, 0] == ind_id]\n",
    "    ind_yhat = complete_inv_yhat[complete_inv_yhat[:, 0] == ind_id]\n",
    "    ind_view = VIEW_ground_truth_v[VIEW_ground_truth_v[:, 0] == ind_id]\n",
    "    print(ind_y.shape)\n",
    "    print(ind_yhat.shape)\n",
    "    print(ind_view.shape)\n",
    "    print(np.around(ind_y[:, -9:].astype(np.double), decimals=2))\n",
    "    #print(np.around(ind_yhat[:, -9:].astype(np.double), decimals=2))\n",
    "    print(np.around(ind_view[:, [2,3,4,5,6,7]].astype(np.double), decimals=2))\n",
    "end_t = time.time()\n",
    "print(end_t-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.03567838668823242\n",
    "0.029976367950439453"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
