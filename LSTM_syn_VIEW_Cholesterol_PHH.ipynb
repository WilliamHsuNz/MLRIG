{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import generic_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import scipy \n",
    "import math\n",
    "import h5py\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 20)\n",
      "Index(['Unnamed: 0', 'id', 'quarter', 'ag_age', 'ag_sex', 'ag_eth', 'pt_nzdep',\n",
      "       'imp_hxdiab', 'pt_tc_hdl_ratio', 'pt_bps', 'pt_bpd', 'pt_smoke',\n",
      "       'imp_hxcvd', 'imp_hdl', 'imp_ldl', 'imp_tchol', 'PH_BL_LLD_ANY',\n",
      "       'PH_BL_AHT_ANY', 'pt_familyhistory', 'tchdl'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0     id  quarter    ag_age  ag_sex ag_eth  pt_nzdep  imp_hxdiab  \\\n",
      "0           0  10000        0  39.14871  Female    Oth         3           0   \n",
      "1           1  10000        1  39.39871  Female    Oth         3           0   \n",
      "2           2  10000        2  39.64871  Female    Oth         3           0   \n",
      "3           3  10000        3  39.89871  Female    Oth         3           0   \n",
      "4           4  10000        4  40.14871  Female    Oth         3           0   \n",
      "\n",
      "   pt_tc_hdl_ratio    pt_bps    pt_bpd  pt_smoke  imp_hxcvd   imp_hdl  \\\n",
      "0         2.718664  113.6584  84.20549         0          0  1.623589   \n",
      "1         2.718664  113.6584  84.20549         0          0  1.623589   \n",
      "2         2.718664  113.6584  84.20549         0          0  1.623589   \n",
      "3         2.718664  113.6584  84.20549         0          0  1.623589   \n",
      "4         2.718664  113.6584  84.20549         0          0  1.623589   \n",
      "\n",
      "    imp_ldl  imp_tchol  PH_BL_LLD_ANY  PH_BL_AHT_ANY  pt_familyhistory  \\\n",
      "0  2.875279   4.378786              0              0                 0   \n",
      "1  2.875279   4.378786              0              0                 0   \n",
      "2  2.875279   4.378786              0              0                 0   \n",
      "3  2.875279   4.378786              0              0                 0   \n",
      "4  2.875279   4.378786              0              0                 0   \n",
      "\n",
      "      tchdl  \n",
      "0  2.720197  \n",
      "1  2.684570  \n",
      "2  2.484248  \n",
      "3  2.653317  \n",
      "4  2.760329  \n"
     ]
    }
   ],
   "source": [
    "syn_df = pd.read_csv('Synthetic_Data\\simple_syn_combined_0.1sd_6_eth.csv')\n",
    "print(syn_df.shape)\n",
    "print(syn_df.columns)\n",
    "print(syn_df.iloc[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Female' 'Male']\n",
      "['Oth' 'Eur' 'Mao' 'Pac' 'Ind' 'Chi']\n",
      "(140000, 20)\n"
     ]
    }
   ],
   "source": [
    "# check categorical variables\n",
    "print(syn_df.ag_sex.unique())\n",
    "print(syn_df.ag_eth.unique())\n",
    "syn_v = syn_df.values\n",
    "print(syn_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 24)\n",
      "[[10000 0 39.148709999999994 1 0.0 0.0 0.0 0.0 1.0 0.0 3 0 2.718664\n",
      "  113.6584 84.20549 0 0 1.623589 2.875279 4.378786 0 0 0\n",
      "  2.720196946511845]\n",
      " [10000 1 39.398709999999994 1 0.0 0.0 0.0 0.0 1.0 0.0 3 0 2.718664\n",
      "  113.6584 84.20549 0 0 1.623589 2.875279 4.378786 0 0 0\n",
      "  2.6845696488452173]\n",
      " [10000 2 39.648709999999994 1 0.0 0.0 0.0 0.0 1.0 0.0 3 0 2.718664\n",
      "  113.6584 84.20549 0 0 1.623589 2.875279 4.378786 0 0 0\n",
      "  2.4842479478513715]\n",
      " [10000 3 39.898709999999994 1 0.0 0.0 0.0 0.0 1.0 0.0 3 0 2.718664\n",
      "  113.6584 84.20549 0 0 1.623589 2.875279 4.378786 0 0 0\n",
      "  2.653316792905103]\n",
      " [10000 4 40.148709999999994 1 0.0 0.0 0.0 0.0 1.0 0.0 3 0 2.718664\n",
      "  113.6584 84.20549 0 0 1.623589 2.875279 4.378786 0 0 0\n",
      "  2.7603294723172174]]\n"
     ]
    }
   ],
   "source": [
    "# one hot encode categorical variables \n",
    "for i in range(len(syn_v)):\n",
    "    if syn_v[i, 4] == 'Male':\n",
    "        syn_v[i, 4] = 0\n",
    "    else:\n",
    "        syn_v[i, 4] = 1\n",
    "        \n",
    "eth_v = np.zeros((140000, 6))\n",
    "eth_idx = {'Chi': 0,\n",
    "           'Eur': 1,\n",
    "           'Ind': 2,\n",
    "           'Mao': 3,\n",
    "           'Oth': 4,\n",
    "           'Pac': 5}\n",
    "\n",
    "for i in range(len(syn_v)):\n",
    "    eth_v[i, eth_idx[syn_v[i, 5]]] = 1\n",
    "\n",
    "syn_v_1 = syn_v[:, 1:5]\n",
    "syn_v_2 = syn_v[:, 6:]\n",
    "syn_encoded_v = np.append(syn_v_1, eth_v, axis=1)\n",
    "syn_encoded_v = np.append(syn_encoded_v, syn_v_2, axis=1)\n",
    "print(syn_encoded_v.shape)\n",
    "print(syn_encoded_v[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whsu014\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# normalise vectors to values between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(syn_encoded_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# set up data for training with generator\n",
    "# build x and y list\n",
    "###############################################\n",
    "x_list = []\n",
    "y_list = []\n",
    "x = np.empty([0, 24])\n",
    "ground_truth = np.empty([0, 24])\n",
    "##################\n",
    "entry_start = 0\n",
    "len_list = [0]\n",
    "len_list_x = [0]\n",
    "len_list_y = [0]\n",
    "id_count = 0\n",
    "##################\n",
    "number_of_inds = 5000\n",
    "for i in range(number_of_inds):\n",
    "    start = i*28\n",
    "    end = (i+1)*28\n",
    "    ind_data = scaled[start:end, :]\n",
    "    ##################################\n",
    "    # create ground truth for forecast\n",
    "    ##################################\n",
    "    id_count += 1\n",
    "    ind_x = ind_data[:-1, :-1]\n",
    "    ind_y = ind_data[1:, -1:]\n",
    "    ind_xy = np.append(ind_x, ind_y, axis=1)\n",
    "    ground_truth = np.append(ground_truth, ind_xy, axis=0)\n",
    "    ##################################\n",
    "    # create data for 1 step forecast\n",
    "    ##################################\n",
    "    for i in range(len(ind_data)-1):\n",
    "        x_list.append(ind_data[:(i+1), :])\n",
    "        y_list.append(ind_data[(i+1), -1:])\n",
    "        if i == len(ind_data)-2:\n",
    "            x = np.append(x, x_list[-1], axis=0)\n",
    "            n_rows = x_list[-1].shape[0]\n",
    "            entry_start += n_rows\n",
    "            len_list.append(entry_start)\n",
    "    # store the indices to fold\n",
    "    # changed from 10 fold to 2 fold \n",
    "    # due to amount of data and fair \n",
    "    # comparison with depmix\n",
    "    if id_count % (number_of_inds//2) == 0 and \\\n",
    "        id_count != 0:\n",
    "            len_list_x.append(len(x_list))\n",
    "            len_list_y.append(len(y_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135000\n",
      "135000\n",
      "3\n",
      "3\n",
      "[0, 67500, 135000]\n",
      "[0, 67500, 135000]\n",
      "2500\n",
      "start 0\n",
      "end 67500\n"
     ]
    }
   ],
   "source": [
    "print(len(x_list))\n",
    "print(len(y_list))\n",
    "print(len(len_list_x))\n",
    "print(len(len_list_y))\n",
    "print(len_list_x)\n",
    "print(len_list_y)\n",
    "#print(len_list)\n",
    "\n",
    "size = len(len_list)//2\n",
    "print(size)\n",
    "print(\"start\", len_list[0])\n",
    "print(\"end\", len_list[size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02541, saving model to weights.hdf5\n",
      " - 233s - loss: 0.0232 - val_loss: 0.0254\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02541 to 0.01809, saving model to weights.hdf5\n",
      " - 227s - loss: 0.0154 - val_loss: 0.0181\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01809 to 0.01580, saving model to weights.hdf5\n",
      " - 227s - loss: 0.0144 - val_loss: 0.0158\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01580 to 0.01513, saving model to weights.hdf5\n",
      " - 232s - loss: 0.0134 - val_loss: 0.0151\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01513 to 0.01392, saving model to weights.hdf5\n",
      " - 230s - loss: 0.0128 - val_loss: 0.0139\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01392 to 0.01328, saving model to weights.hdf5\n",
      " - 228s - loss: 0.0125 - val_loss: 0.0133\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01328 to 0.01324, saving model to weights.hdf5\n",
      " - 230s - loss: 0.0121 - val_loss: 0.0132\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01324 to 0.01273, saving model to weights.hdf5\n",
      " - 233s - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01273 to 0.01269, saving model to weights.hdf5\n",
      " - 225s - loss: 0.0118 - val_loss: 0.0127\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01269 to 0.01216, saving model to weights.hdf5\n",
      " - 226s - loss: 0.0118 - val_loss: 0.0122\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 225s - loss: 0.0115 - val_loss: 0.0128\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 225s - loss: 0.0115 - val_loss: 0.0131\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 224s - loss: 0.0113 - val_loss: 0.0125\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01216 to 0.01172, saving model to weights.hdf5\n",
      " - 229s - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01172 to 0.01149, saving model to weights.hdf5\n",
      " - 225s - loss: 0.0112 - val_loss: 0.0115\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01149 to 0.01138, saving model to weights.hdf5\n",
      " - 230s - loss: 0.0113 - val_loss: 0.0114\n",
      "Epoch 17/50\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# two fold cross validatioin\n",
    "###########################################\n",
    "\n",
    "# keras generators need to be \n",
    "# infinitely iterable\n",
    "def train_generator(x_list, y_list):\n",
    "    # 0.1 validatioin split\n",
    "    train_length = (len(x_list)//10)*9\n",
    "    while True:\n",
    "        for i in range(train_length):\n",
    "            train_x = np.array([x_list[i]])\n",
    "            train_y = np.array([y_list[i]])\n",
    "            yield train_x, train_y\n",
    "            \n",
    "def val_generator(x_list, y_list):\n",
    "    # 0.1 validation split\n",
    "    val_length = len(x_list)//10\n",
    "    while True:\n",
    "        for i in range(-val_length, 0, 1):\n",
    "            val_x = np.array([x_list[i]])\n",
    "            val_y = np.array([y_list[i]])\n",
    "            yield val_x, val_y\n",
    "            \n",
    "start_idx = 0\n",
    "size = len(len_list)//2 #changed from 10\n",
    "\n",
    "rmse_lstm_list = []\n",
    "#rmse_var_list = []\n",
    "rmse_naive_list = []\n",
    "\n",
    "complete_inv_y = np.empty([0, 24])\n",
    "complete_inv_yhat = np.empty([0, 24])\n",
    "#complete_inv_var = np.empty([0, 38])\n",
    "complete_inv_naive = np.empty([0, 24])\n",
    "\n",
    "start_outside = time.time()\n",
    "\n",
    "for f in range(2):\n",
    "    print(\"Fold: \", f)\n",
    "    ##################################\n",
    "    # forecast using LSTM\n",
    "    ##################################\n",
    "    lstm_start = len_list_x[f]\n",
    "    lstm_end = len_list_x[f+1]\n",
    "    \n",
    "    train1_x = x_list[:lstm_start]\n",
    "    train2_x = x_list[lstm_end:]\n",
    "    train_x = train1_x + train2_x\n",
    "    test_x = x_list[lstm_start:lstm_end]\n",
    "    \n",
    "    train1_y = y_list[:lstm_start]\n",
    "    train2_y = y_list[lstm_end:]\n",
    "    train_y = train1_y + train2_y\n",
    "    test_y = y_list[lstm_start:lstm_end]\n",
    "    \n",
    "    # time training\n",
    "    start_time = time.time()\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=False,\n",
    "                   input_shape=(None, 24)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\",\n",
    "                                   monitor='val_loss', verbose=1,\n",
    "                                   save_best_only=True)\n",
    "    history = model.fit_generator(generator=train_generator(train_x,\n",
    "                                                            train_y),\n",
    "                                  steps_per_epoch=(len(train_x)//10)*9,\n",
    "                                  epochs=50,\n",
    "                                  validation_data=val_generator(train_x,\n",
    "                                                                train_y),\n",
    "                                  validation_steps=len(train_x)//10,\n",
    "                                  callbacks=[checkpointer],\n",
    "                                  verbose=2, shuffle=False)\n",
    "    # plot history\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Training LSTM: \", end_time - start_time)\n",
    "    \n",
    "    # LSTM prediction\n",
    "    model.load_weights(\"weights.hdf5\")\n",
    "    yhat = np.empty((0, 1))\n",
    "    for i in range(len(test_x)):\n",
    "        yhat_pred = model.predict(np.array([test_x[i]]))\n",
    "        #print(\"yhat_pred.shape: \", yhat_pred.shape)\n",
    "        yhat = np.append(yhat, yhat_pred, axis=0)\n",
    "    start = len_list[start_idx]\n",
    "    end = len_list[start_idx+size]\n",
    "    xyhat = np.append(x[start:end, :-1], yhat, axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(xyhat)\n",
    "    inv_y = scaler.inverse_transform(ground_truth[start:end, :])\n",
    "    if np.array_equal(inv_y[:, :-1], inv_yhat[:, :-1]):\n",
    "        print(\"inv_y x and inv_yhat x are the same\")\n",
    "    else:\n",
    "        print(\"inv_y x and inv_yhat x are not the same\")\n",
    "    end_time = time.time()\n",
    "    print(\"Training and Testing LSTM: \", end_time - start_time)\n",
    "    #########################################################\n",
    "    # forecasting using Vector Autoregression (VAR) and Naive\n",
    "    #########################################################\n",
    "    rmse_lstm = sqrt(mean_squared_error(inv_y[:, -1:], inv_yhat[:, -1:]))\n",
    "    rmse_lstm_list.append(rmse_lstm)\n",
    "    \n",
    "    complet_inv_y = np.append(complete_inv_y, inv_y, axis=0)\n",
    "    complete_inv_yhat = np.append(complete_inv_yhat, inv_yhat, axis=0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"End of fold \"+str(f)+\":\", end_time - start_time)\n",
    "    start_idx += size\n",
    "print(\"rmse_lstm_list\")\n",
    "end_outside = time.time()\n",
    "print(\"Entire process took:\", end_outside - start_outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
